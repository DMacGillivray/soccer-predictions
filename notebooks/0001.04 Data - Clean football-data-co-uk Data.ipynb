{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0001.01 Data - Scope Football Data Co Uk Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the \"autoreload\" extension\n",
    "%load_ext autoreload\n",
    "# always reload modules marked with \"%aimport\"\n",
    "%autoreload 1\n",
    "# add the 'src' directory to path to import modules\n",
    "src_dir = pathlib.Path().cwd().resolve().parent / 'src'\n",
    "#src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "# import my class code from the source\n",
    "# %aimport src-dir.filename\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "PROJECT_DIR = pathlib.Path().cwd().resolve().parent\n",
    "\n",
    "SCOPED_DATA_DIR = PROJECT_DIR / 'data' / '02-scoped'\n",
    "CLEANED_DATA_DIR = PROJECT_DIR / 'data' / '03-cleaned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation Notes for football-data.co.uk Data Files\n",
    "\n",
    "\n",
    "[Interpretation Notes for football-data-co.uk data files](../data/reference/notes.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the Top 16 European Leagues by Attendance\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_attendance_figures_at_domestic_professional_sports_leagues#Outdoor_sports\n",
    "\n",
    "Referenced on 30 November 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Raw Data Source, League and Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_unnamed(df_orig):\n",
    "    df = df_orig.copy(deep=True)\n",
    "    drop_cols = [col for col in df.columns if 'Unnamed' in col]\n",
    "    df.drop(columns = drop_cols, inplace=True)    \n",
    "    return df\n",
    "\n",
    "def parse_dates(df_orig):\n",
    "    df = df_orig.copy(deep=True)\n",
    "    if 'bad' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "    else:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])    \n",
    "    return df\n",
    "\n",
    "def drop_all_nulls(df_orig, axis=1):\n",
    "    df = df_orig.copy(deep=True)\n",
    "    # Drop any columns that are all null\n",
    "    df = df_orig.dropna(axis=axis, how='all')\n",
    "    return df\n",
    "\n",
    "def drop_unwanted_fdcuk_cols(df_orig):\n",
    "    df = df_orig.copy(deep=True)\n",
    "    unwanted_cols = ['Attendance', 'Country', 'Div', 'Season', 'Time', 'ABP', 'HBP', 'Referee', 'HTR', 'League',\n",
    "                    'LB', 'LB.1', 'LB.2']\n",
    "    df = df.drop(columns=unwanted_cols, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def standardize_duplicate_col_names(df_orig):\n",
    "    # Looks like there are differently named columns for the same thing\n",
    "    # i.e Away, Away team and Home, Home Team\n",
    "    # and for Goals AG and FTAG, and HG and FTHG\n",
    "    df = df_orig.copy(deep=True)\n",
    "    df.rename(columns={'AwayTeam': 'a', 'Away': 'a', 'HomeTeam': 'h', 'Home': 'h'}, inplace=True)\n",
    "    df.rename(columns={'FTHG': 'h_ftGoals', 'HG': 'h_ftGoals'}, inplace=True)\n",
    "    df.rename(columns={'FTAG': 'a_ftGoals', 'AG': 'a_ftGoals'}, inplace=True)\n",
    "    df.rename(columns={'FTR': 'result', 'Res': 'result'}, inplace=True)\n",
    "    \n",
    "    df.rename(columns={'PSH': 'hwinOddsPinn', 'PH': 'hwinOddsPinn'}, inplace=True)\n",
    "    df.rename(columns={'PSD': 'drawOddsPinn', 'PD': 'drawOddsPinn'}, inplace=True)\n",
    "    df.rename(columns={'PSA': 'awinOddsPinn', 'PA': 'awinOddsPinn'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def collect_col_names(df_origs):\n",
    "    cols = []\n",
    "    for df in df_origs:\n",
    "        cols.extend(list(df.columns))\n",
    "    return set(cols)\n",
    "\n",
    "def rename_fdcuk_cols(df_orig):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df_orig.copy(deep=True)\n",
    "    \n",
    "    game_feature_cols = {'AC': 'a_corners', 'AR': 'a_redCards', 'AS': 'a_shots', 'AST': 'a_shotsOnTarget',\n",
    "                         'AY': 'a_yellowCards', 'Date': 'date', 'HC': 'h_corners',\n",
    "                         'HR': 'h_redCards', 'HS': 'h_shots', 'HST': 'h_shotsOnTarget',\n",
    "                         'HY': 'h_yellowCards', 'HTAG': 'a_htGoals', 'HTHG': 'h_htGoals',\n",
    "                         'AF': 'a_fouls', 'HF': 'h_fouls', 'HHW': 'h_woodWork', 'AHW': 'a_woodWork',\n",
    "                         'AO': 'a_offsides', 'HO': 'h_offsides',\n",
    "                        'AFKC': 'a_freeKicksConceded', 'HFKC': 'h_freeKicksConceded'}\n",
    "    \n",
    "    odds_cols = {'B365H': 'hwinOddsBet365', 'B365D': 'drawOddsBet365', 'B365A': 'awinOddsBet365',\n",
    "                'BWH': 'hwinOddsBw', 'BWD': 'drawOddsBw', 'BWA': 'awinOddsBw',\n",
    "                'Bb1X2': 'n_Bb1X2', 'BbAH': 'n_BbAsian', 'BbAHh': 'BbAsianHandicap',\n",
    "                'BbAv<2.5': 'ftGoalsU2.5OddsBbMean', 'BbAv>2.5': 'ftGoalsO2.5OddsBbMean', \n",
    "                'BbAvA': 'awinOddsBbMean',\n",
    "                'BbAvAHA': 'oddsAsianAwayBbMean', 'BbAvAHH': 'oddsAsianHomeBbMean',\n",
    "                'BbAvD': 'drawOddsBbMean', 'BbAvH': 'hwinOddsBbMean',\n",
    "                 'BbMx<2.5': 'ftGoalsU2.5OddsBbMax', 'BbMx>2.5': 'ftGoalsO2.5OddsBbMax',\n",
    "                'BbMxA': 'awinOddsBbMax','BbMxAHA': 'asianAwayOddsBbMax',\n",
    "                 'BbMxAHH': 'asianHomeOddsBbMax',\n",
    "                'BbMxD': 'drawOddsBbMax', 'BbMxH': 'hwinOddsBbMax', 'BbOU': 'n_BbOU',\n",
    "                'IWH': 'hwinOddsIw', 'IWD': 'drawOddsIw', 'IWA': 'awinOddsIw',\n",
    "                'LBH': 'hwinOddsLb', 'LBD': 'drawOddsLb', 'LBA': 'awinOddsLb',\n",
    "                'PSCH': 'hwinClOddsPinn', 'PSCD': 'drawClOddsPinn', 'PSCA': 'awinClOddsPinn',\n",
    "                'VCH': 'hwinOddsVc', 'VCD': 'drawOddsVc', 'VCA': 'awinOddsVc', \n",
    "                'WHH': 'hwinOddsWh', 'WHD': 'drawOddsWh', 'WHA': 'awinOddsWh',\n",
    "                'MaxH': 'hwinOddsMarketMax', 'MaxD': 'drawOddsMarketMax', 'MaxA': 'awayOddsMarketMax',\n",
    "                'AvgH': 'hwinOddsMarketMean', 'AvgD': 'drawOddsMarketMean', 'AvgA': 'awayOddsMarketMean',\n",
    "                'B365AH':'hHandicapSizeBet365', 'B365AHA': 'aAsianHandicapOddsBet365',\n",
    "                'B365AHH': 'hAsianhandicapOddsBet365',\n",
    "                 'BSA': 'awinOddsBsa', 'BSD': 'drawOddsBsa', 'BSH': 'hwinOddsBsa',\n",
    "                'BWA': 'awinOddsBwa', 'BWD': 'drawOddsBwa', 'BWH': 'hwinOddsBwa',\n",
    "                'GBH': 'hwinOddsGb', 'GBD': 'drawOddsGb', 'GBA': 'awinOddsGb',\n",
    "                'SBA': 'awinOddsSb', 'SBD': 'drawOddsSb', 'SBH': 'hwinOddsSb',\n",
    "                'SJA': 'awinOddsSj', 'SJD': 'drawOddsSj', 'SJH': 'hwinOddsSj',\n",
    "                'SOA': 'awinOddsSo', 'SOD': 'drawOddsSo', 'SOH': 'hwinOddsSo',\n",
    "                'SYA': 'awinOddsSy', 'SYD': 'drawOddsSy', 'SYH': 'hwinOddsSy',\n",
    "                 'GBAH': 'hHandicapSizeGb',\n",
    "                 'GBAHA': 'aAsianHandicapOddsGb', 'GBAHH': 'hAsianHandicapOddsGb',\n",
    "                 'LBAH': 'hhandicapSizeLb',\n",
    "                 'LBAHA': 'aAsianHandicapOddsLb', 'LBAHH': 'hAsianHandicapOddsLb',\n",
    "                 'B365<2.5': 'ftGoalsU2.5OddsBet365', 'B365>2.5': 'ftGoalsO2.5OddsBet365',\n",
    "                 'GB<2.5': 'ftGoalsU2.5OddsGb', 'GB>2.5': 'ftGoalsO2.5OddsGb'}\n",
    "    \n",
    "    df.rename(columns=game_feature_cols, inplace=True)\n",
    "    df.rename(columns=odds_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_results_col(df_orig):\n",
    "    \"\"\"\n",
    "    COMMON !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \"\"\"\n",
    "    df = df_orig.copy(deep=True)\n",
    "    try:\n",
    "        if 'result' in df.columns:\n",
    "            df.drop(columns=['result'], inplace=True)\n",
    "            # Calculate Results column\n",
    "        conditions = [df['h_ftGoals'] > df['a_ftGoals'],\n",
    "                      df['h_ftGoals'] == df['a_ftGoals'],\n",
    "                      df['h_ftGoals'] < df['a_ftGoals']]\n",
    "        choices = ['hwin', 'draw', 'awin']\n",
    "        df['result'] = np.select(conditions, choices, default='not-played')\n",
    "    except:\n",
    "        # Where there are abandoned matches or penalty finishes this fails\n",
    "        df['result'] = None\n",
    "    return df  \n",
    "\n",
    "def lowercase_team_names(df_orig):\n",
    "    \"\"\"\n",
    "    COMMON !!!!!!!!!!!!!!\n",
    "    \"\"\"\n",
    "    df = df_orig.copy(deep=True)\n",
    "    for col in ['h', 'a']:\n",
    "        df[col] = df[col].str.lower().str.replace(' ', '-').str.replace('.','')\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_fps(top_level_dir, ext='csv'):\n",
    "    src_fps = list(top_level_dir.rglob('*.' + ext))\n",
    "    return src_fps\n",
    "\n",
    "def read_csvs(fps):\n",
    "    dfs = [pd.read_csv(fp) for fp in fps]\n",
    "    return dfs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_save_fps(top_level_dir, season_dfs, source = 'indatabet-com'):\n",
    "    \"\"\"\n",
    "    COMMON !!!!!!!!!!!!\n",
    "    \"\"\"\n",
    "    \n",
    "    scoped_fdcuk_fps = []\n",
    "    for season_df in season_dfs:\n",
    "        nation = season_df['nation'].unique()[0]\n",
    "        league = season_df['league'].unique()[0]\n",
    "        season = season_df['season'].unique()[0]\n",
    "        fn = str(season) + '.csv'\n",
    "        save_fp = top_level_dir / source / nation / league / season / fn\n",
    "        scoped_fdcuk_fps.append(save_fp)\n",
    "    return scoped_fdcuk_fps\n",
    "\n",
    "\n",
    "def save_dfs_to_fps(dfs, fps):\n",
    "    \"\"\"\n",
    "    COMMON !!!!!!!!!!!!!\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    for df, fp in zip(dfs, fps):\n",
    "        if not fp.exists():\n",
    "            fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(fp, index=False)\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "def clean_up_dfs(dfs):\n",
    "    clean_dfs = []\n",
    "    for df in dfs:\n",
    "        df = drop_unnamed(df)\n",
    "        df = parse_dates(df)\n",
    "        df = drop_all_nulls(df, axis=1)\n",
    "        df = drop_all_nulls(df, axis=0)\n",
    "        df = drop_unwanted_fdcuk_cols(df)\n",
    "        df = standardize_duplicate_col_names(df)\n",
    "        df = rename_fdcuk_cols(df)\n",
    "        df = make_results_col(df)\n",
    "        df = lowercase_team_names(df)\n",
    "        clean_dfs.append(df)\n",
    "    return clean_dfs\n",
    "\n",
    "\n",
    "scoped_fps = get_fps(SCOPED_DATA_DIR / 'football-data-co-uk', ext='csv')\n",
    "dfs = read_csvs(scoped_fps)\n",
    "clean_dfs = clean_up_dfs(dfs)\n",
    "cleaned_fdcuk_fps = make_save_fps(CLEANED_DATA_DIR,\n",
    "                                           clean_dfs,\n",
    "                                           source = 'football-data-co-uk')\n",
    "n_saved = save_dfs_to_fps(clean_dfs, cleaned_fdcuk_fps)\n",
    "n_saved\n",
    "\n",
    "# cols = collect_col_names(clean_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_dfs[170].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dfs[170].info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_fdcuk_load_fps(top_level_dir, scope_data):\n",
    "    \"\"\"\n",
    "    Accepts directory names to enable reach into raw data directory\n",
    "    Returns full filepaths of the data files\n",
    "    \"\"\"\n",
    "    fps = []\n",
    "    for nation, league in zip(scope_data['nations'], scope_data['leagues']):\n",
    "        for season in scope_data['seasons']:\n",
    "            fn = season + '.csv'\n",
    "            stub = RAW_DATA_DIR / 'football-data' / nation / league / season\n",
    "            fp = stub / 'football-data-co-uk' / 'season-data' / fn\n",
    "            if fp.exists():\n",
    "                fps.append(fp)\n",
    "    return fps\n",
    "\n",
    "def add_fdcuk_meta_data(df, fp):\n",
    "    strfp = str(fp)\n",
    "    season = fp.stem\n",
    "    nation=strfp.split('/')[11]\n",
    "    league = strfp.split('/')[12]\n",
    "    df['nation'] = nation\n",
    "    df['league'] = league\n",
    "    df['season'] = season\n",
    "    return df\n",
    "\n",
    "def read_badly_formed_csv_to_df(fp):\n",
    "    # https://stackoverflow.com/questions/55188544/pandas-how-to-workaround-error-tokenizing-data\n",
    "    # Skipping line 17: Expected 59 fields in line 17, saw 65\n",
    "    season_df = pd.read_csv(fp, header=None, sep='\\n', encoding=\"ISO-8859-1\")\n",
    "    season_df = season_df[0].str.split(',', expand=True)\n",
    "    season_df.columns = season_df.iloc[0]\n",
    "    season_df = add_fdcuk_meta_data(season_df, fp)\n",
    "    return season_df\n",
    "\n",
    "    \n",
    "\n",
    "def load_fdcuk_fps_as_dfs(fdcuk_fps):   \n",
    "\n",
    "    good_season_dfs=[]\n",
    "    bad_season_dfs = []\n",
    "    for fp in fdcuk_fps:\n",
    "        try:\n",
    "            # Note: dayfirst=True\n",
    "            season_df = pd.read_csv(fp, dayfirst=True, parse_dates=['Date'],\n",
    "                                    engine='python', error_bad_lines=True, encoding=\"ISO-8859-1\")\n",
    "\n",
    "            season_df = add_fdcuk_meta_data(season_df, fp)\n",
    "            good_season_dfs.append(season_df)\n",
    "        except:\n",
    "            season_df = read_badly_formed_csv_to_df(fp)\n",
    "            bad_season_dfs.append(season_df)\n",
    "    \n",
    "    all_season_dfs = good_season_dfs + bad_season_dfs\n",
    "    return all_season_dfs\n",
    "\n",
    "\n",
    "\n",
    "def make_scoped_save_fps(top_level_dir, season_dfs, source = 'indatabet-com'):\n",
    "    \n",
    "    scoped_fdcuk_fps = []\n",
    "    for season_df in season_dfs:\n",
    "        nation = season_df['nation'].unique()[0]\n",
    "        league = season_df['league'].unique()[0]\n",
    "        season = season_df['season'].unique()[0]\n",
    "        fn = str(season) + '.csv'\n",
    "        save_fp = top_level_dir / source / nation / league / season / fn\n",
    "        scoped_fdcuk_fps.append(save_fp)\n",
    "    return scoped_fdcuk_fps\n",
    "\n",
    "\n",
    "def save_dfs_to_fps(dfs, fps):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    for df, fp in zip(dfs, fps):\n",
    "        if not fp.exists():\n",
    "            fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(fp, index=False)\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "fdcuk_fps = make_fdcuk_load_fps(RAW_DATA_DIR, scope_data)\n",
    "all_season_dfs = load_fdcuk_fps_as_dfs(fdcuk_fps)\n",
    "scoped_fdcuk_fps = make_scoped_save_fps(SCOPED_DATA_DIR,\n",
    "                                           all_season_dfs,\n",
    "                                           source = 'football-data-co-uk')\n",
    "n_saved = save_dfs_to_fps(all_season_dfs, scoped_fdcuk_fps)\n",
    "n_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_season_dfs[-1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify scope of data for cleaning\n",
    "source = 'football-data'\n",
    "nation = 'united-kingdom'\n",
    "league = 'english-premier-league'\n",
    "seasons = ['2008-2009', '2009-2010', '2010-2011', '2011-2012', '2012-2013',\n",
    "           '2013-2014', '2014-2015', '2015-2016', '2016-2017', '2017-2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_fdcu_fps(RAW_DATA_DIR, source, nation, league, seasons):\n",
    "    \"\"\"\n",
    "    Accepts directory names to enable reach into raw data directory\n",
    "    Returns full filepaths of the data files\n",
    "    \"\"\"\n",
    "    fps = []\n",
    "    fdcu_specifics = ['football-data-co-uk', 'season-data']\n",
    "    for season in seasons:\n",
    "        fn = season + '.csv'\n",
    "        stub = RAW_DATA_DIR / source / nation / league / season\n",
    "        fp = stub / fdcu_specifics[0] / fdcu_specifics[1] / fn\n",
    "        #if fp.is_file():\n",
    "        if fp.exists():\n",
    "            fps.append(fp)\n",
    "    return fps\n",
    "\n",
    "fp = form_fdcu_fps(RAW_DATA_DIR, source, nation, league, seasons)\n",
    "fp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nations = ['germany', 'united-kingdom', 'spain', 'italy',\n",
    "          'france', 'united-kingdom', 'germany', 'netherlands',\n",
    "          'russian-federation', 'scotland', 'portugal', 'switzerland',\n",
    "          'belgium', 'turkey', 'poland', 'united-kingdom']\n",
    "\n",
    "# poland, switzerland are multileague\n",
    "\n",
    "leagues = ['bundesliga', 'english-premier-league', 'la-liga', 'serie-a',\n",
    "           'ligue-1', 'english-championship', 'bundesliga-2', 'eredivisie',\n",
    "           'premier-league', 'premiership', 'primeira-liga', 'super-league',\n",
    "           'first-division-a', 'super-lig', 'ekstraklasa', 'one']\n",
    "\n",
    "seasons = ['2000-2001', '2001-2002', '2002-2003', '2003-2004',\n",
    "           '2004-2005', '2005-2006', '2006-2007', '2007-2008',\n",
    "           '2008-2009', '2009-2010', '2010-2011', '2011-2012',\n",
    "           '2012-2013', '2013-2014', '2014-2015', '2015-2016',\n",
    "           '2016-2017', '2017-2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fps = []\n",
    "for nation, league in zip(nations, leagues):\n",
    "    fps = form_fdcu_fps(RAW_DATA_DIR, source, nation, league, seasons)\n",
    "    all_fps.extend(fps)\n",
    "\n",
    "print(len(all_fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_helper(fp, df):\n",
    "    strfp = str(fp)\n",
    "    season = fp.stem\n",
    "    nation=strfp.split('/')[11]\n",
    "    league = strfp.split('/')[12]\n",
    "    df['nation'] = nation\n",
    "    df['league'] = league\n",
    "    df['season'] = season\n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "good_season_dfs=[]\n",
    "bad_season_dfs = []\n",
    "for fp in all_fps:\n",
    "\n",
    "    try:\n",
    "        # Note: dayfirst=True\n",
    "        season_df = pd.read_csv(fp, dayfirst=True, parse_dates=['Date'],\n",
    "                                engine='python', error_bad_lines=True, encoding=\"ISO-8859-1\")\n",
    "        season_df = read_helper(fp, season_df)\n",
    "        good_season_dfs.append(season_df)\n",
    "    except:\n",
    "        # https://stackoverflow.com/questions/55188544/pandas-how-to-workaround-error-tokenizing-data\n",
    "        # Skipping line 17: Expected 59 fields in line 17, saw 65\n",
    "        season_df = pd.read_csv(fp, header=None, sep='\\n', encoding=\"ISO-8859-1\")\n",
    "        season_df = season_df[0].str.split(',', expand=True)\n",
    "        season_df = read_helper(fp, season_df)\n",
    "        bad_season_dfs.append(season_df)\n",
    "\n",
    "df_orig = pd.concat(good_season_dfs, axis=0, sort=True)    \n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any columns that are all null\n",
    "df_orig = df_orig.dropna(axis=1, how='all')\n",
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_orig.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like there are differently named columns for the same thing\n",
    "# i.e Away, Away team and Home, Home Team\n",
    "# and for Goals AG and FTAG, and HG and FTHG\n",
    "df_orig['AwayTeam'] = df_orig['AwayTeam'].fillna(df_orig['Away'])\n",
    "df_orig['HomeTeam'] = df_orig['HomeTeam'].fillna(df_orig['Home'])\n",
    "df_orig['FTHG'] = df_orig['FTHG'].fillna(df_orig['HG'])\n",
    "df_orig['FTAG'] = df_orig['FTAG'].fillna(df_orig['AG'])\n",
    "df_orig['FTR'] = df_orig['FTR'].fillna(df_orig['Res'])\n",
    "\n",
    "df_orig['PSH'] = df_orig['PSH'].fillna(df_orig['PH'])\n",
    "df_orig['PSD'] = df_orig['PSD'].fillna(df_orig['PD'])\n",
    "df_orig['PSA'] = df_orig['PSA'].fillna(df_orig['PA'])\n",
    "\n",
    "\n",
    "df_orig[['league', 'AwayTeam', 'Away', 'HomeTeam', 'Home', 'FTHG', 'HG', 'FTAG', 'AG', 'FTR', 'Res']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df_orig.drop(columns=['Away', 'Home', 'HG', 'AG', 'Res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like we have got some missing team names\n",
    "# If everything else is mt, these may be mt lines at the bottom of the csvs\n",
    "df_1 = df_orig[df_orig['AwayTeam'].isnull()]\n",
    "df_1.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are full of nans - Drop these\n",
    "df_2 = df_orig[~df_orig['AwayTeam'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we definitely do not want\n",
    "df_2 = df_2.drop(columns=['Attendance', 'Country', 'Div', 'Season', 'Time', 'ABP', 'HBP', 'Referee', 'HTR'])\n",
    "df_2 = df_2.sort_values(by=['nation', 'league', 'season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we definitely want to use better features than just goals\n",
    "# Definitely want to use Shots and Shots on target\n",
    "# These are coded as HS, AS, HST, AST\n",
    "msno.matrix(df_2[['HS', 'AS', 'HST', 'AST']])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pre-sorted by nation, league, season so it appears that the missing blocks are by season.\n",
    "# it also seems like there is less Shots on target data, than there is in the shots data, so, we will\n",
    "# Drop any season that have got missing values for HST, or AST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dfs=[]\n",
    "for (nation, league, season), df in df_2.groupby(by=['nation', 'league', 'season']):\n",
    "    if df[['HST', 'AST']].isnull().sum().sum() == 0:\n",
    "          st_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut = pd.concat(st_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_3 = df_cut.copy(deep=True)\n",
    "df_3 = df_3.dropna(axis=1, how='all')\n",
    "df_3 = df_3.sort_values(by=['nation', 'league', 'season'])\n",
    "df_3.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like there are 42200 potential records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have sorted on nation., league and season\n",
    "# remove the full columns, and review how the nulls fit into the overall dataframe\n",
    "\n",
    "#df_null_cols = df_3[df_3.columns[df_3.isnull().any()]]\n",
    "msno.matrix(df_3)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_full_cols = df_3[df_3.columns[~df_3.isnull().any()]]\n",
    "df_3_full_cols.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be keeping all these so rename\n",
    "rename_d = {'AC': 'a_corners', 'AR': 'a_redCards', 'AS': 'a_shots', 'AST': 'a_shotsOnTarget',\n",
    "           'AY': 'a_yellowCards', 'AwayTeam': 'a',\n",
    "            'Date': 'date', 'FTAG': 'a_ftGoals', 'FTHG': 'h_ftGoals', 'FTR': 'ftResult',\n",
    "            'HC': 'h_corners', 'HR': 'h_redCards', 'HS': 'h_shots', 'HST': 'h_shotsOnTarget',\n",
    "            'HY': 'h_yellowCards', 'HomeTeam': 'h',\n",
    "           'HTAG': 'a_htGoals', 'HTHG': 'h_htGoals', 'AF': 'a_fouls', 'HF': 'h_fouls',\n",
    "           'HHW': 'h_woodWork', 'AHW': 'a_woodWork',\n",
    "           'AO': 'a_offsides', 'HO': 'h_offsides'}\n",
    "df_3 = df_3.rename(columns=rename_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_4 = df_3.copy(deep=True)\n",
    "# df_null_cols = df[df.columns[df.isnull().any()]]\n",
    "msno.matrix(df_4[df_4.columns[df_4.isnull().any()]])\n",
    "plt.show();   \n",
    "# for (nation, league, season), df in df_4.groupby(by=['nation', 'league', 'season']):\n",
    "#     print(nation, league, season)\n",
    "#     df_null_cols = df[df.columns[df.isnull().any()]]\n",
    "#     msno.matrix(df_null_cols, figsize=(14,2))\n",
    "#     plt.show();    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# take alook at the columns with any nulls\n",
    "# A full column has 42200 values\n",
    "my_cols = [col for col in df_4.columns if col not in list(rename_d.values())]\n",
    "df_5 = df_4[my_cols]\n",
    "df_5[df_5.columns[df_5.isnull().any()]].info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each game, it would be ideal if, as well as the match statistics, we could have\n",
    "# Mean odds, max odds, a full set of odds from a bookmaker, the closing odds\n",
    "# 42200 Games\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_dfs=[]\n",
    "for (nation, league, season), df in df_4.groupby(by=['nation', 'league', 'season']):\n",
    "    if df[['BbMxH', 'BbMxD', 'BbMxA', 'B365A', 'B365D', 'B365H']].isnull().sum().sum() == 0:\n",
    "          bb_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_6 = pd.concat(bb_dfs)\n",
    "df_6.info(verbose=True, null_counts=True)\n",
    "#df_6[df_6.columns[~df_6.isnull().any()]].info(verbose=True, null_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any columns with less than threshold\n",
    "thresh = int(len(df_6) * 0.75)\n",
    "df_7 = df_6.copy(deep=True)\n",
    "df_7 = df_7.dropna(axis=1, thresh=thresh)\n",
    "df_7.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_7, df_6[['PSCA', 'PSCD', 'PSCH']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in df.columns if not col.startswith(('h', 'a'))]#'h' not in col[0:2] or 'a' not in col[0:1]]\n",
    "#or (col[0] != 'a')]\n",
    "print(sorted(cols))\n",
    "#df_7.columns#info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {'B365H': 'odds_hwin_bet365', 'B365D': 'odds_draw_bet365', 'B365A': 'odds_awin_bet365',\n",
    "            'BWH': 'odds_hwin_BW', 'BWD': 'odds_draw_BW', 'BWA': 'odds_awin_BW',\n",
    "            'Bb1X2': 'n_Bb1X2', 'BbAH': 'n_BbAsian', 'BbAHh': 'BbAsian_handicap',\n",
    "            'BbAv<2.5': 'odds_ftgoalsu2.5_bbmean', 'BbAv>2.5': 'odds_ftgoalso2.5_bbmean', \n",
    "            'BbAvA': 'odds_awin_bbmean',\n",
    "            'BbAvAHA': 'odds_asianaway_bbmean', 'BbAvAHH': 'odds_asianhome_bbmean',\n",
    "            'BbAvD': 'odds_draw_bbmean', 'BbAvH': 'odds_hwin_bbmean',\n",
    "            'BbMx<2.5': 'odds_ftgoalsu2.5_bbmax', 'BbMx>2.5': 'odds_ftgoalso2.5_bbmax',\n",
    "            'BbMxA': 'odds_awin_bbmax','BbMxAHA': 'odds_asianaway_bbmax', 'BbMxAHH': 'odds_asianhome_bbmax',\n",
    "            'BbMxD': 'odds_draw_bbmax', 'BbMxH': 'odds_hwin_bbmax', 'BbOU': 'n_BbOU',\n",
    "            'IWH': 'odds_hwin_IW', 'IWD': 'odds_draw_IW', 'IWA': 'odds_awin_IW',\n",
    "            'LBH': 'odds_hwin_LB', 'LBD': 'odds_draw_LB', 'LBA': 'odds_awin_LB',\n",
    "            'PSCH': 'clodds_hwin_pinn', 'PSCD': 'clodds_draw_pinn', 'PSCA': 'clodds_away_pinn',\n",
    "            'VCH': 'odds_hwin_VC', 'VCD': 'odds_draw_VC', 'VCA': 'odds_awin_VC', \n",
    "            'WHH': 'odds_hwin_WH', 'WHD': 'odds_draw_WH', 'WHA': 'odds_awin_WH'}\n",
    "df.rename(columns=columns, inplace=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cols = df[df.columns[~df.isnull().any()]].columns\n",
    "list(full_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df[df.columns[df.isnull().any()]])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['league'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (nation, league, season), seas_df in df.groupby(by=['nation', 'league', 'season']):\n",
    "    season_df = seas_df.copy(deep=True)\n",
    "    season_df.sort_values(by=['date'], inplace=True)\n",
    "    season_df.reset_index(drop=True, inplace=True)\n",
    "    fn = season + '.csv'\n",
    "    source = 'football-data-co-uk'\n",
    "    save_dir = CLEANED_DIR / source / nation / league / season\n",
    "    save_fp = save_dir / fn\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    season_df.to_csv(save_fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(save_fp, parse_dates=['date'], index_col=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to clean up the data, we have to see all the seaons together, so we can see \n",
    "# what data is missing across the full scope of the data\n",
    "# Therefore, compile everything together and take a look\n",
    "leagues = ['english-premier-league', 'la-liga', 'bundesliga', 'serie-a', 'ligue-1', 'primeira-liga',\n",
    "           'russian-premier-league', 'ukranian-premier-league', 'eredivisie', 'superleague', 'super-lig',\n",
    "           'superliga', 'belgian-pro-league'\n",
    "all_fps = []\n",
    "for league in leagues:\n",
    "fps = form_fdcu_fps(RAW_DATA_DIR, source, nation, league, seasons)\n",
    "season_dfs=[]\n",
    "for season, fp in zip(seasons, fps):\n",
    "    # Note: dayfirst=True\n",
    "    season_df = pd.read_csv(fp, dayfirst=True, parse_dates=['Date'])\n",
    "    # Add the season to help navigate the merged dataframe\n",
    "    season_df['season'] = season\n",
    "    season_dfs.append(season_df)\n",
    "\n",
    "df = pd.concat(season_dfs, axis=0, sort=True)\n",
    "df = df.sort_values(by='Date')\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Complete, Partially Complete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df[df.columns[~df.isnull().any()]]\n",
    "df_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_nulls = df[df.columns[df.isnull().any()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_with_nulls)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like the data is missing based on the season\n",
    "# The Pinnacle closing odds data PSCA, PSCD, PSCH is missing for the first few seasons which is disappointing\n",
    "# But even with missing values, we will leave move this data onto the next stage\n",
    "# BbAH, BBAvAHA, BBAvAHH, BbMxAHA, and BbMXAHH have got more than 99% of their data \n",
    "# ie 3410/3420 = 0.997, so we will keep these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_threshold = 0.995\n",
    "# df.info().index\n",
    "# #droppers = ['BSA']\n",
    "df.dropna(thresh=df.shape[0]*0.6,how='all',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = pd.read_csv(INTERIM_DATA_DIR / league / save_file_name,\n",
    "                      parse_dates=['date'], index_col=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the first data file and take a look\n",
    "fp1 = form_fdcu_fps(RAW_DATA_DIR, source, nation, league, seasons)[0]\n",
    "fp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league = 'english-premier-league'\n",
    "GAME_DATA_DIR = RAW_DATA_DIR / 'football-data' / 'United-Kingdom' / league\n",
    "\n",
    "# seasons 2009-2010 through to 2017-2018\n",
    "start_year = 2009 ; end_year = 2018\n",
    "seasons = [str(year) + '-' + str(year+1) for year in range(start_year, end_year)]\n",
    "print(GAME_DATA_DIR)\n",
    "save_file_name = str('football-data-' + league + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Seasons into one file\n",
    "season_dfs=[]\n",
    "for season in seasons:\n",
    "    season_fp = GAME_DATA_DIR / season / 'football-data-co-uk' / 'season-data' / str(season + '.csv')\n",
    "#     season_dfs = pd.read_csv(season_fp)\n",
    "#     print(season_df.head())\n",
    "    season_df = pd.read_csv(season_fp, dayfirst=True, parse_dates=['Date'])\n",
    "    season_df['season'] = season\n",
    "    season_dfs.append(season_df)\n",
    "\n",
    "df = pd.concat(season_dfs, axis=0, sort=True)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_df = df.copy(deep=True)\n",
    "\n",
    "# Drop unnecesary columns\n",
    "# Referee is not available for the full data set, and Half Time Result and Full Time Result can be calculated\n",
    "seasons_df.drop(columns=['Div', 'FTR', 'HTR', 'Referee', ], inplace=True)\n",
    "seasons_df.rename(columns={'Date': 'date', 'HomeTeam': 'h', 'AwayTeam': 'a',\n",
    "                          'FTHG': 'h_ftgoals', 'FTAG': 'a_ftgoals', 'HTHG': 'h_htgoals', 'HTAG': 'a_htgoals',\n",
    "                         'HS': 'h_shots', 'AS': 'a_shots', 'HST': 'h_sot', 'AST': 'a_sot',\n",
    "                         'HF': 'h_fouls', 'AF': 'a_fouls', 'HC': 'h_corners', 'AC': 'a_corners',\n",
    "                         'HY': 'h_ycards', 'AY': 'a_ycards', 'HR': 'h_rcards', 'AR': 'a_rcards'}, inplace=True)\n",
    "\n",
    "columns = {'B365H': 'odds_hwin_bet365', 'B365D': 'odds_draw_bet365', 'B365A': 'odds_awin_bet365',\n",
    "          'BWH': 'odds_hwin_BW', 'BWD': 'odds_draw_BW', 'BWA': 'odds_awin_BW',\n",
    "          'GBH': 'odds_hwin_GB', 'GBD': 'odds_draw_GB', 'GBA': 'odds_awin_GB',\n",
    "          'IWH': 'odds_hwin_IW', 'IWD': 'odds_draw_IW', 'IWA': 'odds_awin_IW',\n",
    "          'LBH': 'odds_hwin_LB', 'LBD': 'odds_draw_LB', 'LBA': 'odds_awin_LB',\n",
    "          'SBH': 'odds_hwin_SB', 'SBD': 'odds_draw_SB', 'SBA': 'odds_awin_SB',\n",
    "          'WHH': 'odds_hwin_WH', 'WHD': 'odds_draw_WH', 'WHA': 'odds_awin_WH',\n",
    "          'SJH': 'odds_hwin_SJ', 'SJD': 'odds_draw_SJ', 'SJA': 'odds_awin_SJ',\n",
    "          'VCH': 'odds_hwin_VC', 'VCD': 'odds_draw_VC', 'VCA': 'odds_awin_VC',           \n",
    "          'BSH': 'odds_hwin_BS', 'BSD': 'odds_draw_BS', 'BSA': 'odds_awin_BS',\n",
    "          'PSH': 'odds_hwin_pinn', 'PSD': 'odds_draw_pinn', 'PSA': 'odds_awin_pinn',\n",
    "           'Bb1X2': 'n_Bb1X2',\n",
    "           'BbMxH': 'odds_hwin_bbmax', 'BbMxD': 'odds_draw_bbmax', 'BbMxA': 'odds_awin_bbmax',\n",
    "           'BbAvH': 'odds_hwin_bbmean', 'BbAvD': 'odds_draw_bbmean', 'BbAvA': 'odds_awin_bbmean',\n",
    "           'BbOU': 'n_BbOU',\n",
    "           'BbMx>2.5': 'odds_ftgoalso2.5_bbmax', 'BbAv>2.5': 'odds_ftgoalso2.5_bbmean',\n",
    "           'BbMx<2.5': 'odds_ftgoalsu2.5_bbmax', 'BbAv<2.5': 'odds_ftgoalsu2.5_bbmean',\n",
    "           'BbAH': 'n_BbAsian',\n",
    "           'BbAHh': 'BbAsian_handicap',\n",
    "           'BbMxAHH': 'odds_asianhome_bbmax', 'BbAvAHH': 'odds_asianhome_bbmean',\n",
    "           'BbMxAHH': 'odds_asianhome_bbmax', 'BbAvAHH': 'odds_asianhome_bbmean',\n",
    "           'BbMxAHA': 'odds_asianaway_bbmax', 'BbAvAHH': 'odds_asianaway_bbmean',\n",
    "           'BbMxAHA': 'odds_asianaway_bbmax', 'BbAvAHA': 'odds_asianaway_bbmean',\n",
    "           'PSCH': 'clodds_hwin_pinn', 'PSCD': 'clodds_draw_pinn', 'PSCA': 'clodds_away_pinn'}\n",
    "seasons_df.rename(columns=columns, inplace=True)\n",
    "\n",
    "\n",
    "seasons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_df.to_csv(INTERIM_DATA_DIR / league / save_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Back Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INTERIM_DATA_DIR / league / save_file_name,\n",
    "                      parse_dates=['date'], index_col=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
